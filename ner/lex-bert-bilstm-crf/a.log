nohup: ignoring input
Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing LEBertSequenceTagging: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing LEBertSequenceTagging from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LEBertSequenceTagging from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LEBertSequenceTagging were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['bilstm.bias_hh_l0', 'bilstm.bias_ih_l0_reverse', 'crf.trans_matrix', 'bilstm.weight_ih_l0', 'bilstm.bias_ih_l0', 'linear.bias', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'crf.end_trans', 'crf.start_trans', 'linear.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of LEBertSequenceTagging were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized because the shapes did not match:
- bert.embeddings.word_embeddings.weight: found shape torch.Size([21128, 768]) in the checkpoint and torch.Size([21155, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/chenghuadong.chd/.conda/envs/py3.8_torch/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-03-27 16:17:25,085 - INFO - train : total step 1 epoch 0 : loss 247.5711212158203 - (112)
2023-03-27 16:17:26,611 - INFO - train : total step 2 epoch 0 : loss 192.23748779296875 - (112)
2023-03-27 16:17:28,088 - INFO - train : total step 3 epoch 0 : loss 122.88606262207031 - (112)
2023-03-27 16:17:29,646 - INFO - train : total step 4 epoch 0 : loss 102.7718505859375 - (112)
2023-03-27 16:17:31,298 - INFO - train : total step 5 epoch 0 : loss 113.28083801269531 - (112)
2023-03-27 16:17:32,839 - INFO - train : total step 6 epoch 0 : loss 100.29141998291016 - (112)
2023-03-27 16:17:34,330 - INFO - train : total step 7 epoch 0 : loss 103.72477722167969 - (112)
2023-03-27 16:17:35,907 - INFO - train : total step 8 epoch 0 : loss 100.24874877929688 - (112)
2023-03-27 16:17:37,489 - INFO - train : total step 9 epoch 0 : loss 100.12088775634766 - (112)
2023-03-27 16:17:38,955 - INFO - train : total step 10 epoch 0 : loss 98.51923370361328 - (112)
2023-03-27 16:17:40,521 - INFO - train : total step 11 epoch 0 : loss 93.66429138183594 - (112)
2023-03-27 16:17:42,066 - INFO - train : total step 12 epoch 0 : loss 99.0487060546875 - (112)
2023-03-27 16:17:43,617 - INFO - train : total step 13 epoch 0 : loss 95.71591186523438 - (112)
2023-03-27 16:17:45,187 - INFO - train : total step 14 epoch 0 : loss 99.58187866210938 - (112)
2023-03-27 16:17:46,698 - INFO - train : total step 15 epoch 0 : loss 102.95513916015625 - (112)
2023-03-27 16:17:48,275 - INFO - train : total step 16 epoch 0 : loss 92.24420928955078 - (112)
2023-03-27 16:17:49,792 - INFO - train : total step 17 epoch 0 : loss 95.56817626953125 - (112)
2023-03-27 16:17:51,427 - INFO - train : total step 18 epoch 0 : loss 98.90829467773438 - (112)
2023-03-27 16:17:51,856 - INFO - train : total step 19 epoch 0 : loss 88.2808837890625 - (112)
2023-03-27 16:17:53,464 - INFO - train : total step 20 epoch 1 : loss 96.68106842041016 - (112)
2023-03-27 16:17:55,081 - INFO - train : total step 21 epoch 1 : loss 92.53684997558594 - (112)
2023-03-27 16:17:56,634 - INFO - train : total step 22 epoch 1 : loss 105.95026397705078 - (112)
2023-03-27 16:17:58,095 - INFO - train : total step 23 epoch 1 : loss 90.06442260742188 - (112)
2023-03-27 16:17:59,559 - INFO - train : total step 24 epoch 1 : loss 95.86357879638672 - (112)
2023-03-27 16:18:01,143 - INFO - train : total step 25 epoch 1 : loss 108.41651916503906 - (112)
2023-03-27 16:18:02,600 - INFO - train : total step 26 epoch 1 : loss 87.11695861816406 - (112)
2023-03-27 16:18:04,107 - INFO - train : total step 27 epoch 1 : loss 88.61188507080078 - (112)
2023-03-27 16:18:05,605 - INFO - train : total step 28 epoch 1 : loss 99.37139892578125 - (112)
2023-03-27 16:18:07,169 - INFO - train : total step 29 epoch 1 : loss 82.42886352539062 - (112)
2023-03-27 16:18:08,715 - INFO - train : total step 30 epoch 1 : loss 94.25680541992188 - (112)
2023-03-27 16:18:10,330 - INFO - train : total step 31 epoch 1 : loss 94.54985046386719 - (112)
2023-03-27 16:18:11,852 - INFO - train : total step 32 epoch 1 : loss 105.7142333984375 - (112)
2023-03-27 16:18:13,474 - INFO - train : total step 33 epoch 1 : loss 105.32392883300781 - (112)
2023-03-27 16:18:15,055 - INFO - train : total step 34 epoch 1 : loss 101.10525512695312 - (112)
2023-03-27 16:18:16,540 - INFO - train : total step 35 epoch 1 : loss 92.82795715332031 - (112)
2023-03-27 16:18:18,037 - INFO - train : total step 36 epoch 1 : loss 94.85153198242188 - (112)
2023-03-27 16:18:19,522 - INFO - train : total step 37 epoch 1 : loss 93.44287109375 - (112)
2023-03-27 16:18:19,946 - INFO - train : total step 38 epoch 1 : loss 59.194759368896484 - (112)
2023-03-27 16:18:21,461 - INFO - train : total step 39 epoch 2 : loss 89.10415649414062 - (112)
2023-03-27 16:18:22,962 - INFO - train : total step 40 epoch 2 : loss 99.84058380126953 - (112)
2023-03-27 16:18:24,432 - INFO - train : total step 41 epoch 2 : loss 85.82487487792969 - (112)
2023-03-27 16:18:26,014 - INFO - train : total step 42 epoch 2 : loss 83.44965362548828 - (112)
2023-03-27 16:18:27,580 - INFO - train : total step 43 epoch 2 : loss 100.7103271484375 - (112)
2023-03-27 16:18:29,117 - INFO - train : total step 44 epoch 2 : loss 89.29312133789062 - (112)
2023-03-27 16:18:30,586 - INFO - train : total step 45 epoch 2 : loss 88.89749145507812 - (112)
2023-03-27 16:18:32,065 - INFO - train : total step 46 epoch 2 : loss 94.35214233398438 - (112)
2023-03-27 16:18:33,868 - INFO - train : total step 47 epoch 2 : loss 91.146728515625 - (112)
2023-03-27 16:18:35,440 - INFO - train : total step 48 epoch 2 : loss 100.8645248413086 - (112)
2023-03-27 16:18:36,936 - INFO - train : total step 49 epoch 2 : loss 99.61309814453125 - (112)
2023-03-27 16:18:38,407 - INFO - train : total step 50 epoch 2 : loss 95.56376647949219 - (112)
2023-03-27 16:18:39,987 - INFO - train : total step 51 epoch 2 : loss 91.15425109863281 - (112)
2023-03-27 16:18:41,477 - INFO - train : total step 52 epoch 2 : loss 95.05919647216797 - (112)
2023-03-27 16:18:43,081 - INFO - train : total step 53 epoch 2 : loss 107.07083892822266 - (112)
2023-03-27 16:18:44,655 - INFO - train : total step 54 epoch 2 : loss 107.42044067382812 - (112)
2023-03-27 16:18:46,144 - INFO - train : total step 55 epoch 2 : loss 103.97381591796875 - (112)
2023-03-27 16:18:47,720 - INFO - train : total step 56 epoch 2 : loss 93.60693359375 - (112)
2023-03-27 16:18:48,133 - INFO - train : total step 57 epoch 2 : loss 60.953033447265625 - (112)
2023-03-27 16:18:49,671 - INFO - train : total step 58 epoch 3 : loss 99.71027374267578 - (112)
2023-03-27 16:18:51,146 - INFO - train : total step 59 epoch 3 : loss 93.39747619628906 - (112)
2023-03-27 16:18:52,668 - INFO - train : total step 60 epoch 3 : loss 94.03610229492188 - (112)
2023-03-27 16:18:54,180 - INFO - train : total step 61 epoch 3 : loss 99.81076049804688 - (112)
2023-03-27 16:18:55,657 - INFO - train : total step 62 epoch 3 : loss 90.15669250488281 - (112)
2023-03-27 16:18:57,163 - INFO - train : total step 63 epoch 3 : loss 90.87884521484375 - (112)
2023-03-27 16:18:58,706 - INFO - train : total step 64 epoch 3 : loss 93.67402648925781 - (112)
2023-03-27 16:19:00,237 - INFO - train : total step 65 epoch 3 : loss 96.7854995727539 - (112)
2023-03-27 16:19:01,685 - INFO - train : total step 66 epoch 3 : loss 93.66136169433594 - (112)
2023-03-27 16:19:03,220 - INFO - train : total step 67 epoch 3 : loss 93.31645965576172 - (112)
2023-03-27 16:19:04,703 - INFO - train : total step 68 epoch 3 : loss 99.99725341796875 - (112)
2023-03-27 16:19:06,276 - INFO - train : total step 69 epoch 3 : loss 102.72447967529297 - (112)
2023-03-27 16:19:07,823 - INFO - train : total step 70 epoch 3 : loss 91.05293273925781 - (112)
2023-03-27 16:19:09,411 - INFO - train : total step 71 epoch 3 : loss 99.11376190185547 - (112)
2023-03-27 16:19:10,879 - INFO - train : total step 72 epoch 3 : loss 82.70492553710938 - (112)
2023-03-27 16:19:12,475 - INFO - train : total step 73 epoch 3 : loss 94.28941345214844 - (112)
2023-03-27 16:19:14,023 - INFO - train : total step 74 epoch 3 : loss 98.58001708984375 - (112)
2023-03-27 16:19:15,589 - INFO - train : total step 75 epoch 3 : loss 91.4029541015625 - (112)
2023-03-27 16:19:16,066 - INFO - train : total step 76 epoch 3 : loss 120.75086975097656 - (112)
